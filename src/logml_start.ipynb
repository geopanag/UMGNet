{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import json \n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Causal Estimates with Limited Supervision\n",
    "In this notebook we:\n",
    "\n",
    "- Utilize a graph neural network to predict the effect of a marketing campaign to the user's consumption.\n",
    "\n",
    "- Evaluate the impact of supervision i.e. how many users have already participated in the campaign and what was the observed outcome.\n",
    "\n",
    "- We compare our model with standard causal machine learning methods.\n",
    "\n",
    "- We evaluate the method in terms of both uplift and accuracy metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  (user, buys, product)={\n",
       "    edge_index=[2, 14543339],\n",
       "    treatment=[14543339],\n",
       "  },\n",
       "  user={\n",
       "    x=[180653, 7],\n",
       "    t=[180653],\n",
       "    y=[180653],\n",
       "  },\n",
       "  products={ num_products=40542 }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(\"/Users/georgepanagopoulos/Desktop/research/causal_inference/data/retailhero/processed/data.pt\")[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class BipartiteSAGE2mod(torch.nn.Module):\n",
    "    def __init__(self, nfeat:int, nproduct:int , hidden_channels:int , out_channels: int, num_layers:int, dropout_rate:float =0):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        self.user_embed = nn.Linear(nfeat, hidden_channels )\n",
    "        self.item_embed =  nn.Linear(nproduct, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(SAGEConv((-1,-1), hidden_channels))\n",
    "            \n",
    "        #self.lin_hetero = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.hidden_common1 = nn.Linear(hidden_channels + num_layers*hidden_channels, hidden_channels)\n",
    "        self.hidden_common2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.hidden_control = nn.Linear(hidden_channels, int(hidden_channels/2))\n",
    "        self.hidden_treatment = nn.Linear(hidden_channels, int(hidden_channels/2))\n",
    "\n",
    "        self.out_control = nn.Linear( int(hidden_channels/2), out_channels)\n",
    "        self.out_treatment = nn.Linear( int(hidden_channels/2), out_channels)\n",
    "\n",
    "        #self.lin = Linear(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        #self.bn_hidden = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        #self.bn_out = nn.BatchNorm1d(nfeat + hidden_channels + hidden_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, xu: torch.tensor, xp:torch.tensor, edge_index:torch._tensor):\n",
    "        out = [] \n",
    "        xu = self.user_embed(xu)\n",
    "        xp = self.item_embed(xp)\n",
    "\n",
    "        out.append(xu)\n",
    "\n",
    "        embeddings = torch.cat((xu,xp), dim=0) \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            embeddings = self.activation(self.convs[i](embeddings, edge_index))\n",
    "            #embeddings = self.dropout(embeddings)\n",
    "            #embeddings = self.bn_hidden(embeddings)\n",
    "            \n",
    "            out.append(embeddings[:xu.shape[0]])            \n",
    "        \n",
    "        out = torch.cat( out, dim=1)\n",
    "        \n",
    "        hidden = self.dropout(self.activation(self.hidden_common1(out)))\n",
    "        hidden = self.dropout(self.activation(self.hidden_common2(hidden)))\n",
    "        \n",
    "        # separate treatment and control \n",
    "        hidden_1t0 = self.dropout(self.activation(self.hidden_control(hidden)))\n",
    "        hidden_1t1 = self.dropout(self.activation(self.hidden_treatment(hidden)))\n",
    "\n",
    "        out_2t0 = self.activation(self.out_control(hidden_1t0))\n",
    "        out_2t1 = self.activation(self.out_treatment(hidden_1t1))\n",
    "        \n",
    "        \n",
    "        return out_2t1, out_2t0, hidden_1t1, hidden_1t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def outcome_regression_loss(t_true: torch.tensor,y_treatment_pred: torch.tensor, y_control_pred: torch.tensor, y_true: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Compute mse for treatment and control output layers using treatment vector for masking out the counterfactual predictions\n",
    "    \"\"\"\n",
    "    loss0 = torch.mean((1. - t_true) * F.mse_loss(y_control_pred.squeeze(), y_true, reduction='none')) \n",
    "    loss1 = torch.mean(t_true *  F.mse_loss(y_treatment_pred.squeeze(), y_true, reduction='none') )\n",
    "\n",
    "    return loss0 + loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_score(prediction, treatment, target, rate=0.2) -> float:\n",
    "    \"\"\"\n",
    "    From https://ods.ai/competitions/x5-retailhero-uplift-modeling/data\n",
    "    Order the samples by the predicted uplift. \n",
    "    Calculate the average ground truth outcome of the top rate*100% of the treated and the control samples.\n",
    "    Subtract the above to get the uplift. \n",
    "    \"\"\"\n",
    "    order = np.argsort(-prediction)\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "    score = treatment_p - control_p\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/georgepanagopoulos/Desktop/research/causal_inference/code/UMGNet/src/config_RetailHero.json', 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        \n",
    "n_hidden = config[\"n_hidden\"]\n",
    "no_layers = config[\"no_layers\"]\n",
    "out_channels = config[\"out_channels\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "lr = config[\"lr\"]\n",
    "results_file_name = config['results_file_name']\n",
    "model_file_name = config[\"model_file\"]\n",
    "early_thres = config['early_stopping_threshold']\n",
    "l2_reg = config['l2_reg']\n",
    "with_lp = config['with_label_prop'] == 1\n",
    "number_of_runs = config['number_of_runs']\n",
    "dropout = config['dropout']\n",
    "k = 10\n",
    "seed = 1\n",
    "validation_fraction = 5\n",
    "patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "xp = torch.eye(data['products']['num_products']).to(device)\n",
    "xu = data['user']['x'].to(device)\n",
    "treatment = data['user']['t'].to(device)\n",
    "outcome = data['user']['y'].to(device)\n",
    "\n",
    "edge_index = data['user','buys','product']\n",
    "\n",
    "criterion = outcome_regression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=abs(k), shuffle=True, random_state=seed)\n",
    "result_fold = []\n",
    "if with_lp:   \n",
    "    dummy_product_labels = torch.zeros([xp.shape[0],1]).to(device).squeeze()\n",
    "\n",
    "for train_indices, test_indices in kf.split(xu):\n",
    "    test_indices, train_indices = train_indices, test_indices\n",
    "    break \n",
    "\n",
    "# split the test indices to test and validation \n",
    "val_indices = train_indices[:int(len(train_indices)/validation_fraction)]\n",
    "train_indices = train_indices[int(len(train_indices)/validation_fraction):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the graph for treated edges and training users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep the graph before the treatment and ONLY the edges of the the train nodes (i.e. after the treatment)\n",
    "mask = torch.isin(data['user','buys','product']['edge_index'][0, :], torch.tensor(train_indices) )\n",
    "edge_index_up_current = data['user','buys','product']['edge_index'][ : , (~data['user','buys','product']['treatment']) | (mask) ]\n",
    "\n",
    "edge_index_up_current[1] = edge_index_up_current[1]+ data['user']['x'].shape[0]\n",
    "\n",
    "edge_index_up_current = torch.cat([edge_index_up_current,edge_index_up_current.flip(dims=[0])],dim=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import torch_geometric as pyg\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "def train(mask: torch.tensor, model:torch.nn.Module, xu: torch.tensor, xp: torch.tensor, edge_index: torch.tensor, treatment: torch.tensor, outcome: torch.tensor,\n",
    "                  optimizer: Optimizer, criterion: Callable[[torch.tensor, torch.tensor, torch.tensor, torch.tensor], torch.tensor] ):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "      out_treatment, out_control, hidden_treatment, hidden_control = model(xu, xp, edge_index)\n",
    "      loss = criterion(treatment[mask], out_treatment[mask], out_control[mask], outcome[mask])\n",
    "     \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "\n",
    "def test(mask: torch.tensor, model:torch.nn.Module, xu: torch.tensor, xp: torch.tensor, edge_index: torch.tensor, treatment: torch.tensor, outcome: torch.tensor,\n",
    "                  criterion: Callable[[torch.tensor, torch.tensor, torch.tensor, torch.tensor], torch.tensor] ):\n",
    "      model.eval()\n",
    "      out_treatment, out_control, hidden_treatment, hidden_control = model(xu, xp, edge_index)\n",
    "      loss = criterion(treatment[mask], out_treatment[mask], out_control[mask], outcome[mask])\n",
    "      return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = BipartiteSAGE2mod(xu.shape[1], xp.shape[1] , n_hidden, out_channels, no_layers, dropout).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay = l2_reg)\n",
    "\n",
    "# init params\n",
    "out = model( xu, xp , edge_index_up_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"\"\n",
    "early_stopping = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = np.inf\n",
    "print_per_epoch = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(train_indices, model, xu, xp, edge_index_up_current, treatment, outcome, optimizer, criterion)\n",
    "    val_loss = test(val_indices, model, xu, xp, edge_index_up_current, treatment, outcome, criterion)\n",
    "\n",
    "    train_losses.append(float(train_loss.item())) \n",
    "    val_losses.append(float(val_loss.item()))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        early_stopping=0\n",
    "        best_val_acc = val_loss\n",
    "        torch.save(model, model_file)\n",
    "    else:\n",
    "        early_stopping += 1\n",
    "        if early_stopping > patience:\n",
    "            print(\"early stopping..\")\n",
    "            break\n",
    "            \n",
    "    if epoch%print_per_epoch==0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, Tra: {train_loss:.4f}, Val: {val_loss:.4f}') #, Test: {test_acc:.4f}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_file).to(device)\n",
    "model.eval()\n",
    "\n",
    "out_treatment, out_control, hidden_treatment, hidden_control = model(data['user']['xu'], xp, edge_index_up_current)\n",
    "\n",
    "test_loss = criterion(data['user']['t'][mask], out_treatment[mask], out_control[mask], data['user'][\"y\"][mask])\n",
    "\n",
    "treatment_test = data['user']['t'][test_indices].detach().cpu().numpy()\n",
    "outcome_test = data['user']['y'][test_indices].detach().cpu().numpy()\n",
    "out_treatment = out_treatment.detach().cpu().numpy()\n",
    "out_control = out_control.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "uplift = out_treatment[test_indices] - out_control[test_indices]\n",
    "uplift = uplift.squeeze()\n",
    "\n",
    "#mse = (uplift.mean() - (outcome_test[treatment_test==1].mean() - outcome_test[treatment_test==0].mean()))**2\n",
    "print(f'mse {test_loss}')\n",
    "up40 = uplift_score(uplift, treatment_test, outcome_test,0.4)\n",
    "print(f'up40 {up40}')\n",
    "up20 = uplift_score(uplift, treatment_test, outcome_test,0.2)\n",
    "print(f'up20 {up20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.meta import BaseXClassifier, BaseSClassifier, BaseTClassifier,BaseRClassifier, BaseDRRegressor, BaseXRegressor, BaseSRegressor, BaseTRegressor, BaseRRegressor\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from causalml.inference.tree import UpliftTreeClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from causalml.inference.tree.causal.causaltree import CausalTreeRegressor\n",
    "\n",
    "train_indices = torch.cat((train_indices, val_indices), dim=0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = BaseTRegressor(learner = XGBRegressor())\n",
    "\n",
    "learner.fit(X= xu[train_indices], y=outcome[train_indices], treatment= treatment[train_indices] )  \n",
    "uplift=learner.predict(X = xu[train_indices], treatment = treatment[test_indices]).squeeze()\n",
    "\n",
    "uplift=learner.predict(X = xu[test_indices], treatment= treatment[test_indices]).squeeze()\n",
    "\n",
    "score40 = uplift_score(uplift, np.hstack([treatment[train_indices] ,treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.4)\n",
    "score20 = uplift_score(uplift, np.hstack([treatment[train_indices],treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.2)\n",
    "print(f'T-learner up40:{score40},{score20}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_model = ElasticNetPropensityModel() #dic_mod[model_class]()\n",
    "\n",
    "propensity_model.fit(X=xu[train_indices], y = treatment[train_indices])\n",
    "p_train = propensity_model.predict(X=xu[train_indices])\n",
    "p_test = propensity_model.predict(X=xu[test_indices])\n",
    "\n",
    "learner = BaseXRegressor(learner = XGBRegressor())\n",
    "\n",
    "learner.fit(X= xu[train_indices], y=outcome[train_indices], treatment= treatment[train_indices], p=p_train )  \n",
    "\n",
    "uplift=learner.predict(X = xu[test_indices], treatment= treatment[test_indices]).squeeze()\n",
    "\n",
    "score40 = uplift_score(uplift, np.hstack([treatment[train_indices] ,treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.4)\n",
    "score20 = uplift_score(uplift, np.hstack([treatment[train_indices],treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.2)\n",
    "print(f'X-learner up40:{score40},{score20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = CausalTreeRegressor(control_name=\"0\")\n",
    "X_train = np.hstack(( treatment[train_indices].reshape(-1, 1), xu[train_indices]))\n",
    "X_test = np.hstack((treatment[test_indices].reshape(-1, 1),  xu[test_indices]))\n",
    "learner.fit( X = X_train, treatment = treatment[train_indices].astype(str), y=outcome[train_indices])\n",
    "uplift = learner.predict( X = X_test).squeeze()\n",
    "\n",
    "score40 = uplift_score(uplift, np.hstack([treatment[train_indices] ,treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.4)\n",
    "score20 = uplift_score(uplift, np.hstack([treatment[train_indices],treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.2)\n",
    "print(f'X-learner up40:{score40},{score20}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
